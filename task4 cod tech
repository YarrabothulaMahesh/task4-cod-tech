import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Embedding, Dense
import numpy as np
import openai

# Using GPT-4 API (Requires OpenAI API Key)
def generate_text_gpt(prompt, model="gpt-4", max_tokens=100):
    openai.api_key = "YOUR_OPENAI_API_KEY"
    response = openai.ChatCompletion.create(
        model=model,
        messages=[{"role": "system", "content": "You are a helpful assistant."},
                  {"role": "user", "content": prompt}],
        max_tokens=max_tokens
    )
    return response["choices"][0]["message"]["content"].strip()

# LSTM-Based Text Generation

def prepare_data(corpus, seq_length=5):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(corpus)
    sequences = tokenizer.texts_to_sequences(corpus)
    vocab_size = len(tokenizer.word_index) + 1
    
    input_sequences = []
    for seq in sequences:
        for i in range(1, len(seq)):
            input_sequences.append(seq[:i+1])
    
    max_length = max(len(seq) for seq in input_sequences)
    input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding='pre')
    X, y = input_sequences[:, :-1], input_sequences[:, -1]
    y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)
    
    return X, y, tokenizer, max_length, vocab_size


def build_lstm_model(vocab_size, max_length):
    model = Sequential([
        Embedding(vocab_size, 50, input_length=max_length-1),
        LSTM(100, return_sequences=True),
        LSTM(100),
        Dense(vocab_size, activation='softmax')
    ])
    
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model


def generate_text_lstm(seed_text, model, tokenizer, max_length, num_words=20):
    for _ in range(num_words):
        encoded = tokenizer.texts_to_sequences([seed_text])[0]
        encoded = pad_sequences([encoded], maxlen=max_length-1, padding='pre')
        predicted_index = np.argmax(model.predict(encoded), axis=-1)
        output_word = ""
        for word, index in tokenizer.word_index.items():
            if index == predicted_index:
                output_word = word
                break
        seed_text += " " + output_word
    return seed_text

# Example usage (choose GPT or LSTM)
if __name__ == "__main__":
    # Example with GPT
    print("GPT Output:")
    print(generate_text_gpt("Write a paragraph about artificial intelligence."))
    
    # Example with LSTM
    corpus = ["Artificial intelligence is transforming the world of technology.",
              "Machine learning and deep learning are subsets of AI.",
              "Natural language processing enables computers to understand human language."]
    X, y, tokenizer, max_length, vocab_size = prepare_data(corpus)
    lstm_model = build_lstm_model(vocab_size, max_length)
    lstm_model.fit(X, y, epochs=100, verbose=0)  # Train the model
    
    print("\nLSTM Output:")
    print(generate_text_lstm("Artificial intelligence", lstm_model, tokenizer, max_length))
